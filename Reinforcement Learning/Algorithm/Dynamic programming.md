다이나믹(dynamic)은 연속적으로 발생되는 문제들을 푸는 것이고, 프로그래밍(programming)은 개발 언어가 아니라 수학적인 문제를 의미합니다. 따라서 다니나믹 프로그래밍(dynamic programming)은<font color="#ffff00"> 연속적으로 발생되는 문제를 수학적으로 최적화(optimizing)하여 풀어내는 것</font>이라고 할 수 있습니다.

다이나믹 프로그래밍은 [[Markov Decision Process]] 의 모든 상황에 대한 것을 이미 알고 있다고 가정합니다. 그렇기 때문에 계획(planning)이 가능합니다. 어떤 행동을 취했을 때 어떤 상태가 되는지 미리 알고 있기에 계회이 가능한 것과 동일합니다.

MDP와 정책을 입력으로 하여 가치 함수를 찾아내는 것이 <font color="#00b050">예측(prediction) </font>과정입니다. 그리고 MDP를 입력으로 하여 기존 가치 함수를 더욱 최적화하는 것이 <font color="#00b050">컨트롤(control)</font> 과정입니다. 최종적으로 정책은 가치 함수를 사용하여 최적화된 정책을 찾을 수 있습니다.

## <font color="#ffc000">Policy iteration</font>
---
정책을 더 좋게 업데이트하려면 어떻게 해야 할까요? 평가와 발전이라는 두 가지 접근 방식으로 정책을 업데이트할 수 있습니다.

현재 정책을 이용해서 가치 함수를 찾는 것을 <font color="#00b050">평가(evaluate)</font>라고 합니다. 그리고 이 가치 값과 행동에 대한 가치 값을 비교하여 더 좋은 정책을 찾아가는 과정을 <font color="#00b050">발전(improve)</font>이라고 합니다. 이 두 가지 과정을 반복하여 수행하면 정책과 가치는 특정 값으로 수렴하게 되고, 그때가 최적화된 정책과 가치라고 할 수 있습니다.

그럼 먼저 정책 평가를 살펴보겠습니다.

### Policy evaluation
정책 이터레이션(policy iteration)은 정책을 평가하고 발전시켜 나가는 것이 중요합니다. 앞서 언급했듯이 가치함수는 정책이 얼마나 좋은지 판단하는 근거가 됩니다. 어떤 정책을 따라야 더 나은 보상을 받을 수 있는지 알 수 있기 때문입니다.

모든 상태에 대해 그다음 상태가 될 수 있는 행동에 대한 보상의 합을 저장하는 것이 정책 평가(policy evaluation)입니다. 즉, <font color="#ffff00">주변 상태의 가치 함수와 바로 다음 상태에서 얻어지는 보상만 고려해서 현재 상태의 다음 가치 함수를 계산하는 것</font>이라고 할 수 있습니다. 이렇게 계산한 값이 실제 가치 함수 값은 아니지만 무한히 반복하다 보면 어떤 값에 수렴하게 되고, 그 수렴된 값을 실제 가치 함수 값으로 유추할 수 있습니다.

### Policy improvement
정책 발전(policy improvement)으로 가장 많이 알려진 방법이 <font color="#00b050">탐욕적 정책 발전(greedy policy improvement)</font> 입니다. 탐욕적 정책 발전은 에이전트가 할 수 있는 행동들의 행동-가치 함수를 비교하고 가장 큰 함수 값을 가진 행동을 취하는 것입니다. 따라서 탐욕적 정책 발전으로 정책을 업데이트하면 이전 가치 함수에 비해 업데이트된 정책으로 움직였을 때 받을 가치 함수는 무조건 크거나 같고, 장기적으로 최적화된 정책에 수렴할 수 있습니다. 정책 발전에서 사용되는 수식은 다음과 같습니다.

$$\pi'(s) = \arg \max q_{\pi}(s,a)$$

## <font color="#ffc000">Value iteration</font>
---
가치 이터레이션(value iteration)은 최적의 정책을 가정하고 [[Bellman optimality equation]] 을 이용하여 순차적으로 행동을 결정합니다. 가치 이터레이션은 정책 이터레이션과 달리 따로 정책 발전이 필요하지 않습니다. 벨만 최적 방정식으로 문제를 푸는 이 방법은 한 번의 정책 평가 과정을 거치면 최적의 가치 함수와 최적의 정책이 구해지면서 [[Markov Decision Process]] 문제를 풀 수 있기 때문입니다. 가치 이터레이션에서 사용하는 수식은 다음과 같습니다.

![[Pasted image 20231101162141.png]]

