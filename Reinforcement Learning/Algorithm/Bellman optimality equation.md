[[Reinforcement Learning]] 에서 추구하고자 하는 목표는 최적의 가치 함수 값을 찾는 것이 아닌 <font color="#ffff00">최대의 보상을 얻는 정책을 찾는 것</font>입니다. 즉, 최대의 보상을 얻기 위한 정책을 찾기 위해 가치 함수 값이 가장 큰 값을 찾습니다. 또한, 강화 학습에서 어떤 목표를 이루었을 때를 <font color="#00b050">최적(optimal)</font> 의 상태, 즉 어떤 목적이 달성된 상태라고 합니다. 그래서 강화 학습은 목표에 따라 찾은 정책을 <font color="#00b050">최적화된 정책(optimal policy)</font>이라고 하며, 이러한 최적화된 정책을 따르는 벨만 방정식을 <font color="#00b050">벨만 최적 방정식(Bellman optimality equation)</font>이라고 합니다.

## <font color="#ffc000">Optimal value function</font>
---
최적의 가치 함수(Optimal value function)란 <font color="#ffff00">최대의 보상을 갖는 가치 함수</font>입니다. 따라서 다음과 같이 두 가지 함수의 최적화를 구할 수 있습니다.

상태-가치 함수는 어떤 상태가 더 많은 보상을 받을 수 있는지 알려 주며, 행동-가치 함수는 어떤 상태에서 어떤 행동을 취해야 더 많은 보상을 받을 수 있는지 알려줍니다. 따라서 모든 상태에 대해 상태-가치 함수를 계산할 수 있다면, 모든 상태에 대해 최적의 행동(Optimal action)을 선택할 수 있습니다.

최적의 행동을 위한 최적의 상태-가치 함수와 최적의 행동-가치 함수를 알아보겠습니다.

## <font color="#ffc000">Optimal state-value function</font>
---
최적의 상태-가치 함수($v_*(s)$)는 <font color="#ffff00">주어진 모든 정책에 대한 상태-가치 함수의 최댓값</font>이며, 수식은 다음과 같습니다. 
$$v_*(s) = \max v_{\pi}(s)$$

## <font color="#ffc000">Optimal action-value function</font>
---
최적의 상태-가치 함수와 유사하게 최적의 행동-가치 함수($q_*(s,a)$) 는 <font color="#ffff00">주어진 모든 정책에 대해 행동-가치 함수의 최댓값</font>이며, 다음 수식을 사용합니다.

$$q_*(s,a) = \max q_{\pi}(s,a)$$

행동-가치 함수는 현재 사애 $s$ 에서 정책 $\pi$ 를 따라 행동 $a$ 를 했을 때의 가치를 의미합니다.

![[Pasted image 20231101152507.png]]

이때 행동-가치 함수(Q-함수라고도 함)에 대한 최적의 가치 함수를 구할 수 있다면 주어진 상태에서 $q$ 값이 가장 높은 행동을 선택할 수 있게 됩니다. 따라서 최적화된 정책을 구할 수 있습니다. 이렇게 선택된 최적화된 정책은 다음 수식으로 정리할 수 있습니다.

![[Pasted image 20231101160808.png]]

즉, <font color="#ffff00">행동-가치 함수를 최대로 하는 행동만 취하겠다는 의미</font>입니다. 이렇듯 $q_*(s,a)$ 를 찾게 되면 최적화된 정책을 구할 수 있습니다.

