벨만 방정식(Bellman equation)은 앞서 [[Markov Decision Process]] 의 <font color="#ffc000">상태-가치 함수</font>와 <font color="#ffc000">행동-가치 함수</font>의 관계를 나타내는 방정식입니다. 벨만 방정식은 <font color="#00b050">벨만 기대 방정식</font>과 <font color="#00b050">벨만 최적 방정식</font>이 있습니다. 

## <font color="#ffc000">Bellman expectation equation</font>
---
가치 함수 $v_{\pi}(s)$ 는 단순히 어떤 상황이나 미래의 보상을 포함한 가치를 나타냅니다. 다음 상태로 이동하려면 어떤 정책(policy)에 따라 행동해야 하는데, 이때 <font color="#ffff00">정책을 고려한 다음 상태로의 이동이 벨만 기대 방정식(Bellman expectation equation)</font> 입니다.

벨만 기대 방정식으로 상태-가치 함수와 행동-가치 함수를 기댓값 $E$ 로 표현할 수 있습니다. 상태-가치 함수의 벨만 기대 방정식을 알아보기 전에 [[Markov Decision Process]] 의 상태-가치 함수에 대한 도출 과정을 먼저 살펴봅시다.

MDP의 상태-가치 함수에 대한 수식을 다시 써 보겠습니다.
$$v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s]$$

여기에서 $t+2$ 시점부터의 보상 ($\gamma R_{t+2} + \gamma R_{t+3} + \dots$) 을 할인율($\gamma (R_{t+2} + R_{t+3} + \dots)$) 로 묶어 주면, 이 묶인 수식은 $t+1$ 시점부터 받을 보상을 의미합니다. 다시 말해 $t+1$ 시점에서의 가치 함수로 표현할 수 있습니다. 즉, 다음과 같이 다음 상태와 현재 상태의 가치함수 관계를 식으로 나타낼 수 있습니다.

$$v_{\pi}(s) = E_{\pi} [G_t | S_t = s]$$
$$= E_{\pi}[R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s]$$
$$= E_{\pi} [R_{t+1}+\gamma(R_{t+2} + \gamma R_{t+3} + \dots)|S_t = s]$$
$$= E_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$= E_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s]$$

> 기댓값 $E$
> 행동을 어느 방향으로 진행할지 모를때 정의

지금까지 현재 시점의 가치는 현재의 보상과 다음 시점의 가치로 표현할 수 있다는 것을 학습했습니다. 즉, 재귀적인 형태로서 미래의 가치들이 현재의 가치에 영향을 주고 있는 형태라고 이해하면 됩니다.

지금까지 살펴본 벨만 기대 방정식을 좀 더 쉽게 이해할 수 있도록 강화 학습 과정으로 살펴보겠습니다.

1. 처음 에이전트가 접하는 상태 $s$ 나 $a$ 는 임의의 값으로 설정합니다.
2. 환경과 상호작용하면서 얻은 보상과 상태에 대한 정보들을 이용하여 어떤 상태에서 어떤 행동을 취하는 것이 좋은지(최대의 보상을 얻을 수 있는지) 판단합니다.
3. 이때 최적의 행동을 판단하는 수단이 상태-가치 함수와 행동-가치 함수이고, 이것을 벨만 기대 방정식을 이용하여 업데이트하면서 점점 높은 보상을 얻을 수 있는 상태와 행동을 학습합니다.
4. 2~3 의 과정 속에서 최대 보상을 갖는 행동들을 선택하도록 최적화된 정책을 찾습니다.

따라서 벨만 기대 방정식이 갖는 의미는 <font color="#ffff00">미래의 가치 함수 값들을 이용하여 초기화된 임의의 값들을 업데이트하면서 최적의 가치 함수로 다가가는 것</font>입니다. 즉, 강화 학습은 가치 함수 초깃값(0 또는 랜덤한 숫자들)을 2~3의 과정을 반복하며 얻은 정보들로 업데이트하여 최적의 값을 얻는 것입니다. 그리고 이렇게 반복적으로 얻은 값이 가장 클 때 이를 [[Bellman optimality equation]]이라고 합니다.

