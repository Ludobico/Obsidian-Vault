[[Reinforcement Learning]]의 문제들은 마르코프 결정 과정으로 표현하고, 이 마르코프 결정 과정은 모두 마르코프 프로세스에 기반합니다. 따라서 마르코프 프로세스부터 차근차근 학습해 보겠습니다.

## <font color="#ffc000">Markov Decision Process</font>
강화 학습은 마르코프 결정 과정에 학습 개념을 추가한 것이라고 할 수 있습니다. 그러므로 마르코프 결정 과정에 대해 잘 이해하는 것이 강화 학습에서는 중요합니다. 마르코프 결정 과정을 포함하여 강화 학습의 주요 이론을 살펴보겠습니다.

### Markov Process

![[Pasted image 20231031145924.png|500]]

마르코프 프로세스(Markov Process, MP) 는 <font color="#ffff00">어떤 상태가 일정한 간격으로 변하고, 다음 상태는 현재 상태에만 의존하는 확률적 상태 변화를 의미</font>합니다. 즉, 현재 상태에 대해서만 다음 상태가 결정되며, 현재 상태까지의 과정은 전혀 고려할 필요가 없습니다. 이렇게 변화 상태들이 체인처럼 엮어 있다고 하여 <font color="#ffc000">마르코프 체인(Markov Chain)</font>이라고도 합니다.

또 다른 마르코프 프로세스의 정의로는 마르코프 특성(Markov Property)을 지니는 이산 시간(discrete time)에 대한 확률 과정(stochastic process)입니다. 

> 이산 시간(Discrete Time)
> 이산 시간은 시간의 개념을 연속적인 것이 아니라 일정한 간격으로 분리하여 표현하는 시간 개념을 나타냅니다. 이는 연속 시간(Continuous Time)과 대조적입니다.
> 
> 이산 시간은 일정한 시간 간격을 갖는 순간 또는 시점들로 시간을 표현합니다. 이 간격은 보통 규칙적으로 나타납니다. 예를 들어, 초 단위로 이산 시간을 사용하는 경우, 1초 간격마다 시간이 측정됩니다. 따라서 0초,1초,2초,3초,... 과 같이 시간이 분리되어 기록됩니다.

확률 과정은 앞서 살펴보았듯이 <font color="#ffff00">시간에 따라 어떤 사건의 확률이 변화하는 과정</font>을 의미하며, 이산 시간은 <font color="#ffff00">시간이 연속적이 아닌 이산적으로 변함을 의미</font>합니다. 또한, 마르코프 특성은 과거 상태들($S_1, ..., S_{t-1}$) 과 현재 상태($S_t$) 가 주어졌을 때, 미래 상태($S_{t+1}$)는 과거 상태와는 독립적으로 현재 상태로만 결정된다는 것을 의미합니다. 즉, <font color="#ffff00">과거와 현재 상태를 모두 고려했을 때 미래 상태가 나타날 확률과 현재 상태만 고려했을 때 미래 상태가 발생할 확률이 동일하다는 의미</font>입니다. 이것을 수식으로 표현하면 다음과 같습니다.
$$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, ..., S_t)$$
($P$ : 확률)

> | 의 의미
> 수직 막대 기호 | 는 조건부 확률(Conditional Probability)을 나타냅니다. 이 수식은 조건부 확률의 개념을 나타냅니다.
>  $P(A|B)$ 는 조건부 확률로 $B$ 가 발생했을 때 $A$ 가 발생할 확률을 의미합니다.

마르코프 체인은 <font color="#ffff00">시간에 따른 상태 변화</font>를 나타내며, 이때 상태 변화를 전이(transition)라고 합니다. 마르코프 프로세스에서 상태 간 이동인 전이는 확률로 표현하게 되는데, 이를 <font color="#ffc000">상태 전이 확률</font>(state transition probability)이라고 합니다. 즉, 시간 $t$ 에서의 상태를 $s$ 라고 하며, 시간 $t+1$ 에서의 상태를 $s'$ 라고 할 때 상태 전이 확률은 다음과 같이 수식으로 표현할 수 있습니다.



$$P(S_{t+1} = s' | S_t = s) = P_{ss'}$$
($S$ : 상태의 집합, $P_{ss'}$ 상태의 변화를 확률로 표현)

상태 전이 확률은 어떤 상태 $i$ 가 있을 때 그다음 상태 $j$ 가 될 확률을 의미합니다.

![[Pasted image 20231031154719.png]]

예를 들어 위 그림은 병원을 방문한 어느 하루에 대한 마르코프 체인을 표현한 것입니다. 이날의 상태가 병원에서의 <font color="#ffc000">대기</font>와 <font color="#ffc000">진찰</font>, <font color="#ffc000">독서</font>, <font color="#ffc000">웹 서핑</font>, <font color="#ffc000">취침</font>으로만 구성되어 있다고 가정하면, 다섯 가지 상태가 있는 마르코프 프로세스로 표현할 수 있습니다. 이때 하나의 상태에서 다른 상태로 이동할 롹률의 합은 1을 유지한 상태로, 여러 상태가 연쇄적으로 이어져 있습니다. 예를 들어 대기 -> 진찰, 대기-> 독서, 대기 -> 웹서핑으로 진행되는 모든 프로세스의 합은 1이 되어야합니다. 또한, `취침` 은 하루에 끝마치는 종료 상태에 해당됩니다.

그리고 아래 그림은 상태 전이 확률을 행렬 형태로 정리한 것을 <font color="#00b050">상태 전이 확률 행렬(State transition probability matrix)</font>이라고 합니다.

![[Pasted image 20231031163714.png]]

한 가지 주의할 사항은 현재 상태가 바로 이전의 상태로 결정된다는 말을 오해하면 안된다는 점입니다. 내용을 글자 그대로 해석하면 현재 상태가 $A$ 일 때 다음 상태는 $A$ 가 유일하게 결정하는, 다른 경우가 있을 수 없는 것이라고 오해할 수 있습니다. 그러나 핵심은 유일하게 결정한다가 아니라 <font color="#ffff00">현재 상태는 바로 직전의 상태에만 의존한다</font> 는 점입니다. 다시 말해 이전에 아무리 많은 상태가 있었더라도 다음 상태에 영향을 미치는 것은 지금 현재 상태뿐이라는 것을 암시하며, 지금 현재 상태에서 다음 상태를 결정할 때는 여러 가지 확률 변수가 개입하게 됩니다.

