[[Reinforcement Learning]]의 문제들은 마르코프 결정 과정으로 표현하고, 이 마르코프 결정 과정은 모두 마르코프 프로세스에 기반합니다. 따라서 마르코프 프로세스부터 차근차근 학습해 보겠습니다.

## <font color="#ffc000">Markov Decision Process</font>
강화 학습은 마르코프 결정 과정에 학습 개념을 추가한 것이라고 할 수 있습니다. 그러므로 마르코프 결정 과정에 대해 잘 이해하는 것이 강화 학습에서는 중요합니다. 마르코프 결정 과정을 포함하여 강화 학습의 주요 이론을 살펴보겠습니다.

### Markov Process

![[Pasted image 20231031145924.png|500]]

마르코프 프로세스(Markov Process, MP) 는 <font color="#ffff00">어떤 상태가 일정한 간격으로 변하고, 다음 상태는 현재 상태에만 의존하는 확률적 상태 변화를 의미</font>합니다. 즉, 현재 상태에 대해서만 다음 상태가 결정되며, 현재 상태까지의 과정은 전혀 고려할 필요가 없습니다. 이렇게 변화 상태들이 체인처럼 엮어 있다고 하여 <font color="#ffc000">마르코프 체인(Markov Chain)</font>이라고도 합니다.

또 다른 마르코프 프로세스의 정의로는 마르코프 특성(Markov Property)을 지니는 이산 시간(discrete time)에 대한 확률 과정(stochastic process)입니다. 

> 이산 시간(Discrete Time)
> 이산 시간은 시간의 개념을 연속적인 것이 아니라 일정한 간격으로 분리하여 표현하는 시간 개념을 나타냅니다. 이는 연속 시간(Continuous Time)과 대조적입니다.
> 
> 이산 시간은 일정한 시간 간격을 갖는 순간 또는 시점들로 시간을 표현합니다. 이 간격은 보통 규칙적으로 나타납니다. 예를 들어, 초 단위로 이산 시간을 사용하는 경우, 1초 간격마다 시간이 측정됩니다. 따라서 0초,1초,2초,3초,... 과 같이 시간이 분리되어 기록됩니다.

확률 과정은 앞서 살펴보았듯이 <font color="#ffff00">시간에 따라 어떤 사건의 확률이 변화하는 과정</font>을 의미하며, 이산 시간은 <font color="#ffff00">시간이 연속적이 아닌 이산적으로 변함을 의미</font>합니다. 또한, 마르코프 특성은 과거 상태들($S_1, ..., S_{t-1}$) 과 현재 상태($S_t$) 가 주어졌을 때, 미래 상태($S_{t+1}$)는 과거 상태와는 독립적으로 현재 상태로만 결정된다는 것을 의미합니다. 즉, <font color="#ffff00">과거와 현재 상태를 모두 고려했을 때 미래 상태가 나타날 확률과 현재 상태만 고려했을 때 미래 상태가 발생할 확률이 동일하다는 의미</font>입니다. 이것을 수식으로 표현하면 다음과 같습니다.
$$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, ..., S_t)$$
($P$ : 확률)

> | 의 의미
> 수직 막대 기호 | 는 조건부 확률(Conditional Probability)을 나타냅니다. 이 수식은 조건부 확률의 개념을 나타냅니다.
>  $P(A|B)$ 는 조건부 확률로 $B$ 가 발생했을 때 $A$ 가 발생할 확률을 의미합니다.

마르코프 체인은 <font color="#ffff00">시간에 따른 상태 변화</font>를 나타내며, 이때 상태 변화를 전이(transition)라고 합니다. 마르코프 프로세스에서 상태 간 이동인 전이는 확률로 표현하게 되는데, 이를 <font color="#ffc000">상태 전이 확률</font>(state transition probability)이라고 합니다. 즉, 시간 $t$ 에서의 상태를 $s$ 라고 하며, 시간 $t+1$ 에서의 상태를 $s'$ 라고 할 때 상태 전이 확률은 다음과 같이 수식으로 표현할 수 있습니다.



$$P(S_{t+1} = s' | S_t = s) = P_{ss'}$$
($S$ : 상태의 집합, $P_{ss'}$ 상태의 변화를 확률로 표현)

상태 전이 확률은 어떤 상태 $i$ 가 있을 때 그다음 상태 $j$ 가 될 확률을 의미합니다.


