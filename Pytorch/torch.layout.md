`torch.layout` 클래스는 [[Pytorch/torch.Tensor/torch.Tensor|torch.Tensor]] 의 메모리 레이아웃을 나타내는 객체입니다. 현재는 `torch.strided` (밀집 텐서)와 `torch.sparse_coo` (희소 COO 텐서)를 지원하고 있습니다. `torch.strided` 는 가장 일반적으로 사용되는 메모리 레이아웃으로, 각 strided 텐서는 데이터를 저장하는 `torch.Storage` 를 가지고 있습니다. 이 텐서들은 스토리지의 다차원, 스트라이드된 뷰를 제공합니다. 스트라이드는 정수의 리스트로, k번째 스트라이드는 텐서의 k 번째 차원에서 다음 요소로 이동하는데 필요한 메모리 내 점프를 나타냅니다. 이 개념을 통해 많은 텐서 연산을 효율적으로 수행할 수 있습니다.

## example code

```python
import torch
import torch.nn as nn

x = torch.tensor([[1,2,3,4,5], [6,7,8,9,10]])

print(x.stride())

print(x.T.stride())
```

```
(5, 1)
(1, 5)
```

- `stride` 는 각 차원을 따라 다음 요소로 이동하기 위해 건너뛰어야 하는 메모리 위치의 수를 나타냅니다.
- 이 텐서 `x` 는 2차원 텐서로, [[torch.shape]] 는 (2, 5) 입니다.
- `(5,1)` 의 첫번째 값 5는 첫 번째 차원(행)을 따라 이동할 때, 다음 요소로 가기 위해 5칸을 건너뛰야합니다.
- `(5,1)` 의 두번째 값 1은 두 번째 차원(열)을 따라 이동할 때, 다음 요소로 가기위해 1칸만 건너뛰면됩니다.

### Explaning why a transposed tensor's stride becomes (1,5) in pytorch

```python
x = torch.tensor([[1,2,3,4,5], [6,7,8,9,10]])
```

위의 텐서를 출력하면

```
tensor([[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10]])
```

위와 같이 나오고, 전치행렬을 출력하면

```
tensor([[ 1,  6],
        [ 2,  7],
        [ 3,  8],
        [ 4,  9],
        [ 5, 10]])
```

아래와 같이 출력됩니다. 하지만 `stride()` 를 통해 출력된 결과는 **(2,1)이 아닌 (1,5)** 입니다.
이는 [[torch.transpose]] 로 전치를 하면, 논리적으로는 형태가 (5,2)로 변경되지만 [[Pytorch]] 에서는 메모리 효율을 위해 **실제로 데이터를 재배열하지 않습니다.** 대신 [[torch.view]] 를 변경하여 데이터를 다르게 해석합니다.

`(1,5)` 는 **첫번째 차원이 열이되고 두번째 차원이 행**으로 변경되며, 원본 텐서의 첫번째 차원(열)을 이동할때, 1칸만 건너뛰고, 두번째 차원(행)을 이동할때, 5칸을 건너뛰어야합니다.

이는 **원본 데이터의 메모리 레이아웃이 변경되지 않았음을 의미**합니다. 전치 연산은 단순히 데이터를 해석하는 방식만 변경합니다.

실제로 이를 확인하려면 다음과 같이 할 수 있습니다.

```python
x = torch.tensor([[1,2,3,4,5], [6,7,8,9,10]])

print(x.data_ptr())       # 원본 텐서의 메모리 주소
print(x.T.data_ptr())     # 전치된 텐서의 메모리 주소
```

```
2417450029440
2417450029440
```

두 출력값이 동일할 것입니다. 이는 실제 데이터가 이동하지 않았음을 보여줍니다.

이러한 접근 방식은 메모리 사용을 최적화하고 불필요한 복사를 피하는 데 도움이 됩니다. 하지만 때로는 이로 인해 특정 연산의 성능에 영향을 줄 수 있으며, 이럴 때는 [[torch.contiguous]] 메소드를 사용하여 메모리를 재정렬 시킬 수 있습니다.

