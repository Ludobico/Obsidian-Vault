- [[#AutoTokenizer.from_pretrained()|AutoTokenizer.from_pretrained()]]
- [[#Tokenizer Methods|Tokenizer Methods]]
- [[#Tokenizer Methods#encode|encode]]
- [[#Tokenizer Methods#decode|decode]]


AutoTokenizer ëŠ” [[HuggingFaceğŸ¤—]] ì˜ [[transformers]]ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ë„êµ¬ ì¤‘ í•˜ë‚˜ë¡œ, ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì „ì—<font color="#ffff00"> í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜</font>í•´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```


### AutoTokenizer.from_pretrained()
---
AutoTokenizer.from_pretrained() ì€ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ì˜ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  í•´ë‹¹ ëª¨ë°ì— ë§ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•©ë‹ˆë‹¤.

ì£¼ìš” íŒŒë¼ë¯¸í„°ë¡œëŠ”

> pretrained_model_name_or_path -> str or os.PathLike
- ì‚¬ìš©í•  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ ë˜ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	- <font color="#ffff00">ë¬¸ìì—´</font> : [[HuggingFaceğŸ¤—]] ëª¨ë¸ í—ˆë¸Œì—ì„œ í˜¸ìŠ¤íŒ…ë˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ í† í¬ë‚˜ì´ì €ì˜ ëª¨ë¸ idì…ë‹ˆë‹¤. ëª¨ë¸ idëŠ” huggingface.co ì˜ ëª¨ë¸ ë ˆíŒŒì§€í† ë¦¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	- <font color="#ffff00">ë””ë ‰í† ë¦¬ ê²½ë¡œ</font> : í•„ìš”í•œ í† í¬ë‚˜ì´ì €ì˜ ì–´íœ˜ íŒŒì¼ì´ í¬í•¨ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	- <font color="#ffff00">ë‹¨ì–´ ì–´íœ˜ íŒŒì¼ ê²½ë¡œ</font> : í† í¬ë‚˜ì´ì €ê°€ ë‹¨ì¼ ì–´íœ˜ íŒŒì¼ë§Œ í•„ìš”í•œ ê²½ìš° í•´ë‹¹ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> inputs -> (optional)
- [[Tokenizer]] ì˜ `__init__()` ë©”ì„œë“œì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìˆ˜ì…ë‹ˆë‹¤. Tokenizerì˜ ìƒì„±ìì— ì „ë‹¬í•  ì¸ìˆ˜ë¥¼ ì§€ì •í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

> config -> (optional)
- ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•  PretrainedConfig ê°ì²´ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì–´ë–¤ ì¢…ë¥˜ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í• ì§€ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> cache_dir -> str or os.PathLike (optional)
- ë‹¤ìš´ë¡œë“œëœ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì„¤ì •ì„ ìºì‹œí•  ë””ë ‰í† ë¦¬ì˜ ê²½ë¡œì…ë‹ˆë‹¤. í‘œì¤€ ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •í•  ê²½ìš° ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

> force_download -> bool, (optional), Defaults to False
- ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì„¤ì • íŒŒì¼ì„ ê°•ì œ ë‹¤ìš´ë¡œë“œí•˜ë„ë¡ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆë¦¬ì–¸ ê°’ì…ë‹ˆë‹¤. ìºì‹œëœ ë²„ì „ì„ ë¬´ì‹œí•˜ê³  ë‹¤ìš´ë¡œë“œë¥¼ ê°•ì œí•˜ë ¤ë©´ <font color="#ffc000">True</font>ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

> resume_download -> bool, (optional) , Defaults to False
- íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ì— ì¤‘ë‹¨ëœ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ íŒŒì¼ì„ ì‚­ì œí• ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆë¦¬ì–¸ ê°’ì…ë‹ˆë‹¤. ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ <font color="#ffc000">True</font>ë¡œ ì„¤ì •í•˜ë©´ ì¤‘ë‹¨ëœ íŒŒì¼ì´ ìˆì„ ê²½ìš° ë‹¤ìš´ë¡œë“œë¥¼ ì¬ê°œí•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.

> proxies -> dict[str, str] , (optional)
- í”„ë¡ì‹œ ì„œë²„ë¥¼ ì§€ì •í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. í”„ë¡ì‹œëŠ” í”„ë¡œí† ì½œ ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ ë³„ë¡œ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´
```bash
{'http': 'foo.bar:3128', '[http://hostname](http://hostname/)': 'foo.bar:4012'}
```

ì™€ ê°™ì´ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì‹œ í”„ë¡ì‹œ ì„œë²„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> revision -> str , (optional), defaults to "main"
- ì‚¬ìš©í•  ëª¨ë¸ ë²„ì „ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤. ì´ê²ƒì€ <font color="#ffff00">ë¸Œëœì¹˜ ì´ë¦„, íƒœê·¸ ì´ë¦„ ë˜ëŠ” ì»¤ë°‹ ID</font> ê°€ ë  ìˆ˜ ìˆìœ¼ë©°, huggingface.coì˜ ëª¨ë¸ê³¼ ê¸°íƒ€ ì•„í‹°íŒ©íŠ¸ë¥¼ ì €ì¥í•˜ëŠ”ë° Git ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë¯€ë¡œ Gitì—ì„œ í—ˆìš©í•˜ëŠ” ëª¨ë“  ì‹ë³„ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> subfolder -> str, (optional)
- ê´€ë ¨ íŒŒì¼ì´ ìœ„ì¹˜í•œ ì„œë¸Œí´ë”ë¥¼ ì§€ì •í•˜ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ì™€ ê´€ë ¨ëœ íŒŒì¼ì´ íŠ¹ì • ì„œë¸Œí´ë”ì— ì €ì¥ë˜ì–´ ìˆì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

> use_fast -> bool, (optional), Defaults to True
- ì§€ì •ëœ ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš° í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•´ <font color="#ffff00">ë¹ ë¥¸ Rust ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€</font>ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆë¦¬ì–¸ ê°’ì…ë‹ˆë‹¤. ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ì ì¸ Python ê¸°ë°˜ í† í¬ë‚˜ì´ì €ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.

> tokenizer_type -> str, (optional)
- ë¡œë“œí•  í† í¬ë‚˜ì´ì € íƒ€ì…ì…ë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì • ìœ í˜•ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `BertTokenizer` ë‚˜ `GPT2Tokenizer` ì™€ ê°™ì€ í† í¬ë‚˜ì´ì € ìœ í˜•ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•˜ë©´ ìë™ìœ¼ë¡œ ì ì ˆí•œ í† í¬ë‚˜ì´ì € ìœ í˜•ì´ ì„ íƒë©ë‹ˆë‹¤.

> trust_remote_code -> bool, (optional), Defaults to False
- Hubì— ì •ì˜ëœ ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ì‹ ë¢°ì„± ìˆëŠ” ë ˆíŒŒì§€í† ë¦¬ì—ì„œë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, í•´ë‹¹ ë ˆíŒŒì§€í† ë¦¬ì˜ ì½”ë“œë¥¼ í™•ì¸í•œ í›„ì—ë§Œ <font color="#de7802">True</font> ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì´ ì˜µì…˜ì„ <font color="#de7802">True</font> ë¡œ ì„¤ì •í•˜ë©´ Hubì— ìˆëŠ” ì½”ë“œê°€ ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, <font color="#ffff00">ì™¸ë¶€ì—ì„œ ì œê³µë˜ëŠ” ì½”ë“œë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê·¸ì— ë”°ë¼ ì˜µì…˜ì„ ì„¤ì •</font>í•´ì•¼ í•©ë‹ˆë‹¤.

> kwargs -> additional keyword arguments, (optional)
- ì¶”ê°€ì ì¸ í‚¤ì›Œë“œ ì¸ìë¡œ `Tokenizer__init__()` ë©”ì„œë“œë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ `bos_token` `eos_token` `unk_token` `sep_token` `pad_token` ê³¼ ê°™ì€ íŠ¹ìˆ˜ í† í°ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


## Tokenizer Methods
---
`AutoTokenizer.from_pretrained()` ìœ¼ë¡œ [[Tokenizer]] ì¸ìŠ¤í„´ìŠ¤í™” ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Upstage/SOLAR-10.7B-v1.0")
```

ì¸ìŠ¤í„´ìŠ¤ëœ í† í¬ë‚˜ì´ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### encode

ì´ encode ë©”ì„œë“œëŠ” <font color="#ffff00">ì£¼ì–´ì§„ ì…ë ¥ì„ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• </font>ì„ í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” [[Tokenizer]] ì™€ ì–´íœ˜ ì‚¬ì „(vocab dict)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ë¨¼ì € tokenize ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³ , `convert_tokens_to_ids` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì£¼ìš” íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

> text -> str, List[str] or List[int]

- ì²«ë²ˆì§¸ ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤. ë¬¸ìì—´, í† í°í™”ëœ ë¬¸ìì—´ì˜ ë¦¬ìŠ¤íŠ¸, ë˜ëŠ” ì •ìˆ˜ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.

> text_pair -> str, List[str] or List[int], optional
- ë‘ ë²ˆì§¸ ì‹œí€€ìŠ¤ì…ë‹ˆë‹¤.

> add_special_tokens -> bool, optional
- Trueë¡œ ì„¤ì •í•˜ë©´ ì‹œí€€ìŠ¤ì˜ íŠ¹ìˆ˜ í† í°ì´ ì¶”ê°€ë©ë‹ˆë‹¤.

```python
  class TokenizerExam:
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained("Upstage/SOLAR-10.7B-v1.0")
  @classmethod
  def special_token_test(cls):
    tokenizer = cls().tokenizer
    print(tokenizer.encode("I am learning how to use transformers for text generation.", add_special_tokens=False))
    print('-'*50)
    print(tokenizer.encode("I am learning how to use transformers for text generation.", add_special_tokens=True))
```

```
[315, 837, 5168, 910, 298, 938, 5516, 404, 354, 2245, 8342, 28723]
--------------------------------------------------
[1, 315, 837, 5168, 910, 298, 938, 5516, 404, 354, 2245, 8342, 28723]
```


> max_length -> int, optional
- ë°˜í™˜ë˜ëŠ” ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

> stride -> int, optional
- `max_length` ì™€ í•¨ê»˜ ì„¤ì •ëœ ê²½ìš°, ë°˜í™˜ëœ ì˜¤ë²„í”Œë¡œìš° í† í°ì—ëŠ” ì£¼ìš” ì‹œí€€ìŠ¤ì˜ ì¼ë¶€ í† í°ì´ í¬í•¨ë©ë‹ˆë‹¤.

> truncation_strategy -> str, optional
- ì£¼ì–´ì§„ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì˜ë¼ë‚´ëŠ” strategyë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

> pad_to_max_length -> bool, optional
- Trueë¡œ ì„¤ì •í•˜ë©´ ë°˜í™˜ëœ ì‹œí€€ìŠ¤ê°€ ëª¨ë¸ì˜ ìµœëŒ€ ê¸¸ì´ê¹Œì§€ [[padding]] ë©ë‹ˆë‹¤.

> return_tensors -> str, optional
- '<font color="#ffff00">tf' </font>ë˜ëŠ” <font color="#ffff00">'pt'</font> ë¡œ ì„¤ì •í•˜ì—¬ TensorFlowtf.constant ë˜ëŠ” [[Pytorch]] ì˜ [[torch.Tensor]] ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

### decode

ì´ decode ë©”ì„œë“œëŠ” <font color="#ffff00">ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• </font>ì„ í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” í† í¬ë‚˜ì´ì €ì™€ ì–´íœ˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

ì£¼ìš” íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

> token_ids -> List[int]
- í† í°í™”ëœ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ìˆ˜ ID ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ëŠ” `encode` ë˜ëŠ” `encode_plus` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> skip_special_tokens -> bool, default by False
- Trueë¡œ ì„¤ì •í•˜ë©´ íŠ¹ìˆ˜ í† í°ì„ ì œê±°í•˜ê³  ì¼ë°˜ í† í°ë§Œì„ í¬í•¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

> clean_up_tokenization_spaces -> bool, default by False
- Trueë¡œ ì„¤ì •í•˜ë©´ í† í°í™” ê³¼ì •ì—ì„œ ì¶”ê°€ëœ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë” ì •í™•í•œ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

### apply_chat_template

ì±„íŒ… ëª¨ë¸ì˜ í…œí”Œë¦¿ì€ ëª¨ë¸ì´ <font color="#ffff00">ì±„íŒ… í˜•ì‹ì„ ì…ë ¥ì„ ë°›ì„ ë•Œ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ</font>ì…ë‹ˆë‹¤. ê° ì±„íŒ… ëª¨ë¸ì€ ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì…ë ¥ì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì— í…œí”Œë¦¿ì€ ê·¸ì— ë§ê²Œ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì •ì˜í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, FaceBookì˜ BlenderBot ëª¨ë¸ì˜ ê²½ìš° ë§¤ìš° ê°„ë‹¨í•œ ê¸°ë³¸ í…œí”Œë¦¿ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ë³¸ í…œí”Œë¦¿ì€ ì£¼ë¡œ ëŒ€í™”ì˜ ê° ë¼ìš´ë“œ ì‚¬ì´ì— ê³µë°±ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëŒ€í™”ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

chat = [
   {"role": "user", "content": "Hello, how are you?"},
   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
   {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

```
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

ë¬¸ì¥ì—ì„œ ì–¸ê¸‰ëœ ê²ƒì²˜ëŸ¼ ì „ì²´ ì±„íŒ…ì´ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ì••ì¶•ë©ë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì¸ `tokenize=True` ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ ë¬¸ìì—´ë„ í† í°í™”ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¢€ ë” ë³µì¡í•œ í…œí”Œë¦¿ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ê¸°ìœ„í•´ mistralai/Mistral-7B-Instruct-v0.1 ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

chat = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]

tokenizer.apply_chat_template(chat, tokenize=False)
```

```
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

ìœ„ì˜ ì˜ˆì œì—ì„œ ë³´ë“¯ì´, ì±„íŒ… í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì—­í• (role) ê³¼ ë‚´ìš©(content) í‚¤ë¥¼ ê°€ì§„ ë©”ì‹œì§€ ëª©ë¡ì„ ì‘ì„±í•œ ë‹¤ìŒ **apply_chat_template()** ë©”ì„œë“œì— ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì¶œë ¥ì„ ì–»ê²Œ ë©ë‹ˆë‹¤.

ëª¨ë¸ ìƒì„±ì— ì±„íŒ… í…œí”Œë¦¿ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í• ë•Œ, generation promptë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ **add_generation_prompt = True** ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì œì—ì„œëŠ” Zephyr ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **model.generate()** ì— ëŒ€í•œ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```

```
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s> 
<|user|>
How many helicopters can a human eat in one sitting?</s> 
<|assistant|>
```

ì´ì œ inputì´ Zephyrì— ë§ê²Œ ì˜¬ë°”ë¥´ê²Œ í¬ë§·ë˜ì—ˆìœ¼ë¯€ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

ê·¸ëŸ¬ë©´ ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§ˆê²ƒì…ë‹ˆë‹¤.

```
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s> 
<|user|>
How many helicopters can a human eat in one sitting?</s> 
<|assistant|>
Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.
```

ì •ë§ ì‰½ì£ ?

[[HuggingFaceğŸ¤—]] ì—ì„œëŠ” text generation íŒŒì´í”„ë¼ì¸ì€ ì±— ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ í•˜ê¸° ìœ„í•œ ì±„íŒ… inputì„ ì œê³µí•©ë‹ˆë‹¤. ê³¼ê±°ì—ëŠ” ì´ íŒŒì´í”„ë¼ì¸ì„ ConversationalPipeline ì´ë¼ê³  ë¶ˆë €ì§€ë§Œ, í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ë˜ì—ˆê³ , **TextGenerationPipeline** ì´ë¼ëŠ” ëª¨ë“ˆì— ëª¨ë“  ê¸°ëŠ¥ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œí•œë²ˆ Zephyr ëª¨ë¸ë¡œ ì‹¤í—˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response
```

```
{'role': 'assistant', 'content': "Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all."}
```

íŒŒì´í”„ë¼ì¸ì€ í† í°í™” ë° **apply_chat_template** ì— ëŒ€í•œ ëª¨ë“  ì„¸ë¶€ì‚¬í•­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ì±„íŒ… í…œí”Œë¦¿ì´ ìˆìœ¼ë©´ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  message listë¥¼ ì „ë‹¬í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

