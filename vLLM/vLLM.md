
## Overview

vLLM은 LLM의 추론 및 서빙 속도를 극대화하기 위해 설계된 오픈소스 라이브러리입니다. 특히 [[PagedAttention]] 이라는 혁신적인 알고리즘을 도입하여, 기존 방식보다 메모리 효율성을 비약적으로 높이고 처리량(Throughput)을 최대 10~20배까지 향상시킨 것이 특징입니다.

