- [[#Llamap.cpp|Llamap.cpp]]
- [[#Installation|Installation]]
	- [[#Installation#CPU-Only|CPU-Only]]
	- [[#Installation#GPU (cuBLAS, OpenBLAS)|GPU (cuBLAS, OpenBLAS)]]
- [[#Installation on Windows|Installation on Windows]]
- [[#Error handle|Error handle]]
	- [[#Error handle#TypeError: 'NoneType' object is not callable|TypeError: 'NoneType' object is not callable]]
		- [[#TypeError: 'NoneType' object is not callable#langchain|langchain]]
		- [[#TypeError: 'NoneType' object is not callable#llama-cpp-python|llama-cpp-python]]

## Llamap.cpp 

`llama-cpp-python` ì€ `llama.cpp` ë¥¼ [[Python]] ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ë°”ì¸ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. [[HuggingFaceğŸ¤—]] ì— ì˜¬ë¼ì™€ìˆëŠ” ë‹¤ì–‘í•œ LLM ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§€ì›í•˜ë©°, [[LangChain/LangChain|LangChain]] ê³¼ í†µí•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì°¸ê³ ë¡œ `llama-cpp-python` ì˜ ìµœì‹  ë²„ì „ì€ ê¸°ì¡´ GGML í¬ë§·ëŒ€ì‹  [[GGUF]] í¬ë§· íŒŒì¼ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë§Œì¼ **ê¸°ì¡´ GGML ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜**í•˜ë ¤ë©´, `llama.cpp` ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```
python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin
```

## Installation

`llma-cpp-python` ì€ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©í•˜ëŠ” í™˜ê²½(CPU, GPU, Mac) ì— ë”°ë¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### CPU-Only

```
pip install --upgrade --quiet llama-cpp-python
```


### GPU (cuBLAS, OpenBLAS)

`llma.cpp` ëŠ” ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì—¬ëŸ¬ BLAS ë°±ì—”ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
ì•„ë˜ì™€ ê°™ì´ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ GPU ì§€ì›ì„ í™œì„±í™”í•œ ìƒíƒœë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install llama-cpp-python
```

<font color="#ffff00">ì´ì „ì— CPU-Only ë²„ì „ì„ ì„¤ì¹˜í–ˆë‹¤ë©´, ë°˜ë“œì‹œ ì•„ë˜ì²˜ëŸ¼ ê°•ì œ ì¬ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.</font>

```bash
CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir
```

## Installation on Windows

Windowsì—ì„œëŠ” ì†ŒìŠ¤ ì½”ë“œì—ì„œ ì§ì ‘ ë¹Œë“œí•´ì„œ ì„¤ì¹˜í•˜ëŠ” ë°©ì‹ì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.

í•„ìˆ˜ ì¤€ë¹„
> 	git
	python
	cmake
	Visual Studio Community (make sure you install this with the following settings)
		Desktop development with C++
		Python development
		Linux embedded development with C++

ë¦¬í¬ì§€í† ë¦¬ ë³µì œ ë° ì„¤ì •

```bash
git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git
cd llama-cpp-python
```

í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```
set FORCE_CMAKE=1  
set CMAKE_ARGS=-DGGML_CUDA=ON
```

ì„¤ì¹˜ ëª…ë ¹

```bash
python -m pip install -e .
```

<font color="#ffff00">ì´ì „ì— CPU-Only ë²„ì „ì„ ì„¤ì¹˜í–ˆë‹¤ë©´, ë°˜ë“œì‹œ ì•„ë˜ì²˜ëŸ¼ ê°•ì œ ì¬ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.</font>

```bash
python -m pip install -e . --force-reinstall --no-cache-dir
```


## Error handle

### TypeError: 'NoneType' object is not callable

[ê´€ë ¨ ê¹ƒí—ˆë¸Œ ì´ìŠˆ](https://github.com/abetlen/llama-cpp-python/issues/1610)

[[Python]] ì¸í„°í”„ë¦¬í„°ê°€ ì¢…ë£Œë  ë•Œ, ê°ì²´ì™€ ëª¨ë“ˆì˜ ì‚­ì œ ìˆœì„œëŠ” ë³´ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `_LlamaModel.__del__()` ì´ ì‹¤í–‰ë  ë•Œ `llama_cpp` ëª¨ë“ˆì´ ì´ë¯¸ ì‚¬ë¼ì¡Œë‹¤ë©´ `llma_cpp.llama_free.model()` í˜¸ì¶œì´ ì‹¤íŒ¨í•˜ê²Œ ë©ë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ë ¤ë©´ `__del__()` ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , <font color="#ffff00">atexit</font> ëª¨ë“ˆì„ ì‚¬ìš©í•´ ëª…ì‹œì ìœ¼ë¡œ `model.close()` ë¥¼ í˜¸ì¶œí•´ì•¼í•©ë‹ˆë‹¤.

#### langchain

```python
import atexit
from langchain_community.llms import LlamaCpp

model = LlamaCpp(...)

@atexit.register
def free_model():
    model.client.close()
```

#### llama-cpp-python

```python
import atexit
from llama_cpp import Llama

model = Llama(...)

@atexit.register
def free_model():
    model.close()
```
