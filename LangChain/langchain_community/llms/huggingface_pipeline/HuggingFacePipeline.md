`HuggingFacePipeline` ì€ [[HuggingFaceğŸ¤—]] ì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ì„ [[LangChain/LangChain|LangChain]] ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” Huggingface hubì— í˜¸ìŠ¤íŒ…ëœ ìˆ˜ë§ì€ ëª¨ë¸ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`HuggingFacePipeline` ì„ ì‚¬ìš©í•˜ë ¤ë©´ [[transformers]] ë° [[Pytorch]] íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ xformer íŒ¨í‚¤ì§€ë¥¼ ì¶”ê°€ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## example code
---

```python
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline

hf = HuggingFacePipeline.from_model_id(
    model_id="gpt2",
    task="text-generation",
    pipeline_kwargs={"max_new_tokens": 10},
)
```

## example code 2
---

```python
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10)
hf = HuggingFacePipeline(pipeline=pipe)
```

