- [[#Parameters|Parameters]]
- [[#Methods|Methods]]
	- [[#Methods#atransform_documents()|atransform_documents()]]
	- [[#Methods#create_documents()|create_documents()]]
	- [[#Methods#from_huggingface_tokenizer()|from_huggingface_tokenizer()]]
	- [[#Methods#from_tiktoken_encoder()|from_tiktoken_encoder()]]
	- [[#Methods#split_documents()|split_documents()]]
	- [[#Methods#split_text()|split_text()]]
	- [[#Methods#transform_documents()|transform_documents()]]


`TokenTextSplitter` ëŠ” **í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„í• ** í•˜ëŠ” [[LangChain/LangChain|LangChain]] ì˜ [[text_splitter]] í´ëž˜ìŠ¤ìž…ë‹ˆë‹¤.

ì£¼ë¡œ LLMì˜ í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•˜ì—¬ í† í° ê°œìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(
    chunk_size=200,  # ì²­í¬ í¬ê¸°ë¥¼ 10ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    chunk_overlap=0,  # ì²­í¬ ê°„ ì¤‘ë³µì„ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
)

# state_of_the_union í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
texts = text_splitter.split_text(file)
print(texts[0])  # ë¶„í• ëœ í…ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
```


## Parameters

> encoding_name -> str, Default 'gpt2'

- ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €ì˜ ì¸ì½”ë”© ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤.

> model_name -> optional, str

- íŠ¹ì • ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. `gpt-4` ë‚˜ `gpt-3.5-turbo` ì™€ ê°™ì€ ëª¨ë¸ ì´ë¦„ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

> allowd_special -> Union\[Literal\['all'], AbstactSet\[str]]

- í—ˆìš©í•  íŠ¹ìˆ˜ í† í°ì˜ ì§‘í•©ì„ ì§€ì •í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ë©”ì‹œì§€ë‚˜ íŠ¹ì • í† í°ì„ ìœ ì§€í•˜ë ¤ëŠ” ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.

> disallowed_special 

- í—ˆìš©í•˜ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ í† í°ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ `all` ë¡œ ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ í—ˆìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.



## Methods

### atransform_documents()

ì£¼ì–´ì§„ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> documents -> Sequence\[Document\]
- ë³€í™˜í•´ì•¼ í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

> kwargs


### create_documents()

í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ `Document` ê°ì²´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

> text -> List\[str\]
- í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

> medatatas -> optional, List\[dict\]
- ê° í…ìŠ¤íŠ¸ì— ëŒ€ì‘í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸

### from_huggingface_tokenizer()

[[HuggingFaceðŸ¤—]] ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” Text spliiterë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

> tokenizer -> Any
- HuggingFace [[Tokenizer]] ê°ì²´

> kwargs

### from_tiktoken_encoder()

Tiktoken ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” Text splitterë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

> encoding_name -> str
- ì‚¬ìš©í•˜ë ¤ëŠ” ì¸ì½”ë”©ì˜ ì´ë¦„ (ì˜ˆ : gpt2)

> model_name -> optional, str
- ëª¨ë¸ ì´ë¦„

> allowed_special -> Union\[Literal\['all'\]\, AbstractSet\[str\]\]
- í—ˆìš©í•  íŠ¹ìˆ˜ í† í°

> disallowed_special -> Union\[Literal\['all'\]\, AbstractSet\[str\]\]
- í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ í† í°

> kwargs

```python
text_splitter = TokenTextSplitter.from_tiktoken_encoder(encoding_name="gpt2")
```
### split_documents()

ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

> documents -> Iteralble[Document]
- ë¶„í• í•˜ë ¤ëŠ” ë¬¸ì„œì˜ iterable
### split_text()

í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ìš”ì†Œë¡œ ë¶„í• í•©ë‹ˆë‹¤.

> text -> str
- ë¶„í• í•˜ë ¤ëŠ” í…ìŠ¤íŠ¸

```python
split_texts = token_text_splitter.split_text("This is a sample text.")
```
### transform_documents()

ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë¶„í• í•œ ë’¤ ë³€í™˜í•©ë‹ˆë‹¤.

> documents -> Sequence\[Document\]
- ë³€í™˜ ëŒ€ìƒ ë¬¸ì„œì˜ ì‹œí€€ìŠ¤

> kwargs

```python
transformed_docs = token_text_splitter.transform_documents(documents)
```

