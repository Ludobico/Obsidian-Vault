![[Pasted image 20260119130357.png]]

기존의 LLM 추론 방식에서는 [[KV Cache]] 를 위해 연속적인(Contiguous) 메모리 공간을 미리 할당해야 했습니다. 이로 인해 세 가지 문제가 발생합니다.

- Internal Fragmentation : 요청된 문장 길이가 할당된 최대 길이보다 짧을 때 남는 공간이 낭비됨

- External Fragmentation : 메모리 조각들이 흩어져 있어, 합치면 충분한 공간임에도 불구하고 새로운 요을 수용하지 못함

- Reservation : 다음에 생성될 토큰을 위해 미리 공간을 비워두어야 하므로 실제 사용량보다 더 많은 메모가 점유됨

## PagedAttention의 작동 원리

PagedAttention은 [[KV Cache]] 를 연속된 공간이 아닌, **고정된 크기의 블록(Block, 페이지와 유사) 단위**로 쪼개어 비연속적인 물리 메모리에 저장합니다.

![[Pasted image 20260119130706.png]]

### 핵심 구성 요소

1. KV Blocks : kv 캐시 토큰들을 담는 고정 크기 단위

2. Logical KV Blocks : 모델 관점에서의 연속된 토큰 시퀀스

3. Physical KV Blocks : 실제 GPU 메모리에 흩어져 있는 물리적 공간

4. Block Table : 논리적 블록과 물리적 블록의 매핑 정보를 기록하는 표

### 핵심 프로세스

1. 토큰이 생성되면, 현재 블록에 저장합니다.
2. 블록이 꽉 차면, 운영체제가 새로운 페이지를 할당하듯이 물리 메모리의 비어 있는 아무 곳이나 새로운 블록을 할당합니다.
3. Block Table이 이 논리적 순서와 물리적 위치를 연결해 줍니다.
4. Attention 연산 시, 이 테이블을 참조하여 흩어진 블록들을 읽어와 연산합니다.

