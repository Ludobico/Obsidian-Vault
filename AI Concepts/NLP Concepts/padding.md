ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•˜ë‹¤ë³´ë©´ ê° ë¬¸ì¥(ë˜ëŠ” ë¬¸ì„œ)ì€ ì„œë¡œ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ê¸°ê³„ëŠ” ê¸¸ì´ê°€ ì „ë¶€ ë™ì¼í•œ ë¬¸ì„œë“¤ì— ëŒ€í•´ì„œëŠ” í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ ë³´ê³ , í•œêº¼ë²ˆì— ë¬¶ì–´ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´ ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•´ì„œ <font color="#ffff00">ì—¬ëŸ¬ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì„ì˜ë¡œ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”</font>í•©ë‹ˆë‹¤. 

[[Pytorch]] ë° [[HuggingFaceğŸ¤—]] ì˜ [[AutoTokenizer]] ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence

class TokenizerExam:
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained("Upstage/SOLAR-10.7B-v1.0")
  @classmethod
  def solar_tokenizer_sequences_padding(cls, sequence_type : str = "korean"):
    tokenizer = cls().tokenizer
    def get_sequence():
      if sequence_type == "korean":
        sequence = ["ì •ìˆ˜ ì¸ì½”ë”© ì±•í„°ì—ì„œ ìˆ˜í–‰í–ˆë˜ ì‹¤ìŠµì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.","ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ê³ , ì •ìˆ˜ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.","ëª¨ë“  ë‹¨ì–´ê°€ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."]
        return sequence
      elif sequence_type == "english":
        sequence = ["I am learning how to use transformers for text generation. Here is an example of text generation.","Here is an example of text generation with tokenization.","Here is an example of tokenization with text generation."]
        return sequence

    sequence = get_sequence()
    
    tokenized_sequnces = [tokenizer.encode(sequence, add_special_tokens=True) for sequence in sequence]
    # í† í°í™”ëœ ë¬¸ì¥
    print(tokenized_sequnces)
    # í† í°í™” ë¬¸ì¥ ê¸¸ì´
    for seq in tokenized_sequnces:
      print(len(seq))
```

```
[[1, 28705, 29233, 29151, 28705, 29324, 30293, 31836, 28705, 239, 180, 152, 29665, 29148, 29305, 28705, 29151, 30094, 30676, 238, 144, 155, 28705, 29991, 29570, 29189, 28705, 29614, 29634, 29143, 28705, 30192, 30357, 29426, 29477, 31810, 29570, 29194, 29043, 28723, 28705, 29536, 29889, 30275, 28705, 30836, 29015, 28705, 240, 136, 144, 29304, 29404, 28705, 29969, 29015, 29665, 29135, 28705, 29604, 29570, 29194, 29043, 28723], [1, 28705, 30315, 29433, 28705, 31056, 29770, 29189, 28705, 29757, 29939, 29511, 28725, 28705, 29233, 29151, 28705, 29324, 30293, 31836, 29189, 28705, 29151, 30094, 29770, 29194, 29043, 28723], [1, 28705, 29820, 30615, 28705, 30315, 29433, 29135, 28705, 29511, 30127, 29282, 28705, 29233, 29151, 29143, 28705, 29901, 30391, 29848, 30591, 29570, 29194, 29043, 28723]]
64
28
25
```

ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì„ [[Tokenizer]] ì²˜ë¦¬ë¥¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê° ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë¨¸ì§€ë¥¼ padding ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
    padded_sequences = pad_sequence([torch.tensor(tokseq) for tokseq in tokenized_sequnces])
    # í† í°í™”ëœ ë¬¸ì¥
    print(padded_sequences)
    # í† í°í™” ë¬¸ì¥ ê¸¸ì´
    print(padded_sequences.size())
```

```
tensor([[    1,     1,     1],
        [28705, 28705, 28705],
        [29233, 30315, 29820],
        [29151, 29433, 30615],
        [28705, 28705, 28705],
        [29324, 31056, 30315],
        [30293, 29770, 29433],
        [31836, 29189, 29135],
        [28705, 28705, 28705],
        [  239, 29757, 29511],
        [  180, 29939, 30127],
        [  152, 29511, 29282],
        [29665, 28725, 28705],
        [29148, 28705, 29233],
        [29305, 29233, 29151],
        [28705, 29151, 29143],
        [29151, 28705, 28705],
        [30094, 29324, 29901],
        [30676, 30293, 30391],
        [  238, 31836, 29848],
        [  144, 29189, 30591],
        [  155, 28705, 29570],
        [28705, 29151, 29194],
        [29991, 30094, 29043],
        [29570, 29770, 28723],
        [29189, 29194,     0],
        [28705, 29043,     0],
        [29614, 28723,     0],
        [29634,     0,     0],
        [29143,     0,     0],
        [28705,     0,     0],
        [30192,     0,     0],
        [30357,     0,     0],
        [29426,     0,     0],
        [29477,     0,     0],
        [31810,     0,     0],
        [29570,     0,     0],
        [29194,     0,     0],
        [29043,     0,     0],
        [28723,     0,     0],
        [28705,     0,     0],
        [29536,     0,     0],
        [29889,     0,     0],
        [30275,     0,     0],
        [28705,     0,     0],
        [30836,     0,     0],
        [29015,     0,     0],
        [28705,     0,     0],
        [  240,     0,     0],
        [  136,     0,     0],
        [  144,     0,     0],
        [29304,     0,     0],
        [29404,     0,     0],
        [28705,     0,     0],
        [29969,     0,     0],
        [29015,     0,     0],
        [29665,     0,     0],
        [29135,     0,     0],
        [28705,     0,     0],
        [29604,     0,     0],
        [29570,     0,     0],
        [29194,     0,     0],
        [29043,     0,     0],
        [28723,     0,     0]])
torch.Size([64, 3])
```

