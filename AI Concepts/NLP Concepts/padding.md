자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 <font color="#ffff00">여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요</font>합니다. 

[[Pytorch]] 및 [[HuggingFace🤗]] 의 [[AutoTokenizer]] 를 사용하여 아래와 같이 구현할 수 있습니다.

```python
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence

class TokenizerExam:
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained("Upstage/SOLAR-10.7B-v1.0")
  @classmethod
  def solar_tokenizer_sequences_padding(cls, sequence_type : str = "korean"):
    tokenizer = cls().tokenizer
    def get_sequence():
      if sequence_type == "korean":
        sequence = ["정수 인코딩 챕터에서 수행했던 실습을 그대로 반복해보겠습니다. 아래와 같이 텍스트 데이터가 있습니다.","단어 집합을 만들고, 정수 인코딩을 수행합니다.","모든 단어가 고유한 정수로 변환되었습니다."]
        return sequence
      elif sequence_type == "english":
        sequence = ["I am learning how to use transformers for text generation. Here is an example of text generation.","Here is an example of text generation with tokenization.","Here is an example of tokenization with text generation."]
        return sequence

    sequence = get_sequence()
    
    tokenized_sequnces = [tokenizer.encode(sequence, add_special_tokens=True) for sequence in sequence]
    # 토큰화된 문장
    print(tokenized_sequnces)
    # 토큰화 문장 길이
    for seq in tokenized_sequnces:
      print(len(seq))
```

```
[[1, 28705, 29233, 29151, 28705, 29324, 30293, 31836, 28705, 239, 180, 152, 29665, 29148, 29305, 28705, 29151, 30094, 30676, 238, 144, 155, 28705, 29991, 29570, 29189, 28705, 29614, 29634, 29143, 28705, 30192, 30357, 29426, 29477, 31810, 29570, 29194, 29043, 28723, 28705, 29536, 29889, 30275, 28705, 30836, 29015, 28705, 240, 136, 144, 29304, 29404, 28705, 29969, 29015, 29665, 29135, 28705, 29604, 29570, 29194, 29043, 28723], [1, 28705, 30315, 29433, 28705, 31056, 29770, 29189, 28705, 29757, 29939, 29511, 28725, 28705, 29233, 29151, 28705, 29324, 30293, 31836, 29189, 28705, 29151, 30094, 29770, 29194, 29043, 28723], [1, 28705, 29820, 30615, 28705, 30315, 29433, 29135, 28705, 29511, 30127, 29282, 28705, 29233, 29151, 29143, 28705, 29901, 30391, 29848, 30591, 29570, 29194, 29043, 28723]]
64
28
25
```

일반적으로 문장의 길이가 다른 문장들을 [[Tokenizer]] 처리를 하면 다음과 같이 각 문장의 길이가 다르게 나오는데, 여기서 가장 긴 문장을 기준으로 나머지를 padding 처리를 할 수 있습니다.

```python
    padded_sequences = pad_sequence([torch.tensor(tokseq) for tokseq in tokenized_sequnces])
    # 토큰화된 문장
    print(padded_sequences)
    # 토큰화 문장 길이
    print(padded_sequences.size())
```

```
tensor([[    1,     1,     1],
        [28705, 28705, 28705],
        [29233, 30315, 29820],
        [29151, 29433, 30615],
        [28705, 28705, 28705],
        [29324, 31056, 30315],
        [30293, 29770, 29433],
        [31836, 29189, 29135],
        [28705, 28705, 28705],
        [  239, 29757, 29511],
        [  180, 29939, 30127],
        [  152, 29511, 29282],
        [29665, 28725, 28705],
        [29148, 28705, 29233],
        [29305, 29233, 29151],
        [28705, 29151, 29143],
        [29151, 28705, 28705],
        [30094, 29324, 29901],
        [30676, 30293, 30391],
        [  238, 31836, 29848],
        [  144, 29189, 30591],
        [  155, 28705, 29570],
        [28705, 29151, 29194],
        [29991, 30094, 29043],
        [29570, 29770, 28723],
        [29189, 29194,     0],
        [28705, 29043,     0],
        [29614, 28723,     0],
        [29634,     0,     0],
        [29143,     0,     0],
        [28705,     0,     0],
        [30192,     0,     0],
        [30357,     0,     0],
        [29426,     0,     0],
        [29477,     0,     0],
        [31810,     0,     0],
        [29570,     0,     0],
        [29194,     0,     0],
        [29043,     0,     0],
        [28723,     0,     0],
        [28705,     0,     0],
        [29536,     0,     0],
        [29889,     0,     0],
        [30275,     0,     0],
        [28705,     0,     0],
        [30836,     0,     0],
        [29015,     0,     0],
        [28705,     0,     0],
        [  240,     0,     0],
        [  136,     0,     0],
        [  144,     0,     0],
        [29304,     0,     0],
        [29404,     0,     0],
        [28705,     0,     0],
        [29969,     0,     0],
        [29015,     0,     0],
        [29665,     0,     0],
        [29135,     0,     0],
        [28705,     0,     0],
        [29604,     0,     0],
        [29570,     0,     0],
        [29194,     0,     0],
        [29043,     0,     0],
        [28723,     0,     0]])
torch.Size([64, 3])
```

