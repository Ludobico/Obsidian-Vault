## ChatPromptTemaplte
---


```python
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)
```

```python
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
```

`StrOutputParser()` 는 기존의 `invoke()` 함수로 불러왔을때 AIMessage(), HumanMessage()같은 key값들을 안띄우게 만들어줌

## COT Prompting
---
기존의 맥락을 주고 질문하는 기법 [[FewShotPromptTemplate]] 에서 사용하는 것이랑 비슷

[[few_shot]] 으로 COT를 하는것을 FewShotPrompt-CoT라고함

Zero-shot , Zero-shot-CoT 기법도 존재함



## Divide and conquer
---

![[Pasted image 20240517192242.png]]

### example code

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    """
    Please provide three questions from the following text:
    ---
    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, 
    demonstrating superior performance in various natural language processing (NLP) tasks. 
    Inspired by recent efforts to efficiently up-scale LLMs, 
    we present a method for scaling LLMs called depth up-scaling (DUS), 
    which encompasses depthwise scaling and continued pretraining.
    In contrast to other LLM up-scaling methods that use mixture-of-experts, 
    DUS does not require complex changes to train and inference efficiently. 
    We show experimentally that DUS is simple yet effective 
    in scaling up high-performance LLMs from small ones. 
    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, 
    a variant fine-tuned for instruction-following capabilities, 
    surpassing Mixtral-8x7B-Instruct. 
    SOLAR 10.7B is publicly available under the Apache 2.0 license, 
    promoting broad access and application in the LLM field.
    """
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
```


## Prompt engineering guide
---
### References

* https://platform.openai.com/docs/guides/prompt-engineering

* https://docs.anthropic.com/claude/docs/intro-to-prompting

* https://smith.langchain.com/hub


## CAG (Content argumented generation)
---
환각현상 방지를 위해 만든 프롬프트

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)
chain = prompt_template | llm | StrOutputParser()
context = """
We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, 
    demonstrating superior performance in various natural language processing (NLP) tasks. 
    Inspired by recent efforts to efficiently up-scale LLMs, 
    we present a method for scaling LLMs called depth up-scaling (DUS), 
    which encompasses depthwise scaling and continued pretraining.
    In contrast to other LLM up-scaling methods that use mixture-of-experts, 
    DUS does not require complex changes to train and inference efficiently. 
    We show experimentally that DUS is simple yet effective 
    in scaling up high-performance LLMs from small ones. 
    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, 
    a variant fine-tuned for instruction-following capabilities, 
    surpassing Mixtral-8x7B-Instruct. 
    SOLAR 10.7B is publicly available under the Apache 2.0 license, 
    promoting broad access and application in the LLM field.
"""
```

```
If the answer is not present in the context, please write "The information is not present in the context.
```

## Table extraction
---

`pypdf` 로 불러온 pdf 파일에서 표를 인식하지 못하는 에러가 있음

![[Pasted image 20240517194722.png]]

이를 마크다운형태로 변경하여, loader안에 넣는방식

```python
from langchain_upstage import UpstageLayoutAnalysisLoader


layzer = UpstageLayoutAnalysisLoader("pdfs/solar_sample.pdf", output_type="html")
# For improved memory efficiency, consider using the lazy_load method to load documents page by page.
docs = layzer.load()
```

```python
from IPython.display import display, HTML

display(HTML(docs[0].page_content[:5000]))
```

![[Pasted image 20240517200413.png]]

## Agent
---

### Smart rag
- 기존 rag task에서는 문서를 기반으로 답변하고, 문서에 없는 문장이면 모르도록 지시함
- Smart rag는 문서에 있는 질문이 나오면 Rag task를 진행하고, 문서에 없는 질문이면 일반적인 Search를 지시

```python
# RAG or Search?
def is_in(question, context):
    is_in_conetxt = """As a helpful assistant, 
please use your best judgment to determine if the answer to the question is within the given context. 
If the answer is present in the context, please respond with "yes". 
If not, please respond with "no". 
Only provide "yes" or "no" and avoid including any additional information. 
Please do your best. Here is the question and the context:
---
CONTEXT: {context}
---
QUESTION: {question}
---
OUTPUT (yes or no):"""

    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)
    chain = is_in_prompt | ChatUpstage() | StrOutputParser()

    response = chain.invoke({"context": context, "question": question})
    print(response)
    return response.lower().startswith("yes")
```

```python
is_in("How to get to Seoul from SF", solar_summary)
```

```
no

False
```

```python
is_in("What is DUS?", solar_summary)
```

```
yes

True
```

### smart rag with Tavily

```python
# Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient


def smart_rag(question, context):
    if not is_in(question, context):
        print("Searching in tavily")
        tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
        context = tavily.search(query=question)

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

smart_rag("What is DUS?", solar_summary)
```

```
yes

'The answer to the question "What is DUS?" is:\n\nDepth Up-Scaling (DUS)'
```

```python
smart_rag("How to get to Seoul from SF?", solar_summary)
```

```
no.
Searching in tavily

'The answer to "How to get to Seoul from SF?" is:\n\n1. Fly from San Francisco (SFO) to Seoul (ICN) with airlines such as ANA, Japan Airlines, Asiana Airlines, Korean Air, and United Airlines.\n2. Take a train from Incheon Int\'l Airport T1 to Seoul Station.\n3. Take the BART from Civic Center / UN Plaza to Milpitas and then fly from San Jose (SJC) to Incheon (ICN).\n\nPlease note that the cheapest flights from San Francisco to Seoul start at $453 with AIR PREMIA.'
```

## Function calling (tool use)
---
LLM을 호출할때, 질문을 하면 답변을 하기 전에 각 task별 function을 구현하고, 답변에 맞춰 task에 맞는 function을 호출하는 기능

### 10.tool rag.ipynb 참조


```python
from langchain_upstage import ChatUpstage

llm = ChatUpstage()


solar_summary = """
SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling

We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, 
demonstrating superior performance in various natural language processing (NLP) tasks. 
Inspired by recent efforts to efficiently up-scale LLMs, 
we present a method for scaling LLMs called depth up-scaling (DUS), 
which encompasses depthwise scaling and continued pretraining.
In contrast to other LLM up-scaling methods that use mixture-of-experts, 
DUS does not require complex changes to train and inference efficiently. 
We show experimentally that DUS is simple yet effective 
in scaling up high-performance LLMs from small ones. 
Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, 
a variant fine-tuned for instruction-following capabilities, 
surpassing Mixtral-8x7B-Instruct. 
SOLAR 10.7B is publicly available under the Apache 2.0 license, 
promoting broad access and application in the LLM field.
"""

# Tools
from langchain_core.tools import tool
import requests
import os
from tavily import TavilyClient

tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])


@tool
def solar_paper_search(query: str) -> str:
    """Query for research paper about solarllm, dus, llm and general AI.
    If the query is about DUS, Upstage, AI related topics, use this.
    """
    return solar_summary


@tool
def internet_search(query: str) -> str:
    """This is for query for internet search engine like Google.
    Query for general topics.
    """
    return tavily.search(query=query)


@tool
def get_news(topic: str) -> str:
    """Get latest news about a topic.
    If users are more like recent news, use this.
    """
    # https://newsapi.org/v2/everything?q=tesla&from=2024-04-01&sortBy=publishedAt&apiKey=API_KEY
    # change this to request news from a real API
    news_url = f"https://newsapi.org/v2/everything?q={topic}&apiKey={os.environ['NEWS_API_KEY']}"
    respnse = requests.get(news_url)
    return respnse.json()


tools = [solar_paper_search, internet_search, get_news]


llm_with_tools = llm.bind_tools(tools)
```

```python
llm_with_tools.invoke("What is Solar LLM?").tool_calls
```

```
[{'name': 'solar_paper_search',
  'args': {'query': 'Solar LLM'},
  'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]
```

```python
llm_with_tools.invoke("What is top news about Seoul").tool_calls
```

```
[{'name': 'get_news',
  'args': {'topic': 'Seoul'},
  'id': '9f0829a2-da28-4f39-9832-14d07df59eb0'}]
```

