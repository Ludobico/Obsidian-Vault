## ChatPromptTemaplte
---


```python
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)
```

```python
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
```

`StrOutputParser()` 는 기존의 `invoke()` 함수로 불러왔을때 AIMessage(), HumanMessage()같은 key값들을 안띄우게 만들어줌

## COT Prompting
---
기존의 맥락을 주고 질문하는 기법 [[FewShotPromptTemplate]] 에서 사용하는 것이랑 비슷

[[few_shot]] 으로 COT를 하는것을 FewShotPrompt-CoT라고함

Zero-shot , Zero-shot-CoT 기법도 존재함



## Divide and conquer
---

![[Pasted image 20240517192242.png]]

### example code

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    """
    Please provide three questions from the following text:
    ---
    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, 
    demonstrating superior performance in various natural language processing (NLP) tasks. 
    Inspired by recent efforts to efficiently up-scale LLMs, 
    we present a method for scaling LLMs called depth up-scaling (DUS), 
    which encompasses depthwise scaling and continued pretraining.
    In contrast to other LLM up-scaling methods that use mixture-of-experts, 
    DUS does not require complex changes to train and inference efficiently. 
    We show experimentally that DUS is simple yet effective 
    in scaling up high-performance LLMs from small ones. 
    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, 
    a variant fine-tuned for instruction-following capabilities, 
    surpassing Mixtral-8x7B-Instruct. 
    SOLAR 10.7B is publicly available under the Apache 2.0 license, 
    promoting broad access and application in the LLM field.
    """
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
```


## Prompt engineering guide
---
### References

* https://platform.openai.com/docs/guides/prompt-engineering

* https://docs.anthropic.com/claude/docs/intro-to-prompting

* https://smith.langchain.com/hub


## CAG (Content argumented generation)
---
환각현상 방지를 위해 만든 프롬프트

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)
chain = prompt_template | llm | StrOutputParser()
context = """
We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, 
    demonstrating superior performance in various natural language processing (NLP) tasks. 
    Inspired by recent efforts to efficiently up-scale LLMs, 
    we present a method for scaling LLMs called depth up-scaling (DUS), 
    which encompasses depthwise scaling and continued pretraining.
    In contrast to other LLM up-scaling methods that use mixture-of-experts, 
    DUS does not require complex changes to train and inference efficiently. 
    We show experimentally that DUS is simple yet effective 
    in scaling up high-performance LLMs from small ones. 
    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, 
    a variant fine-tuned for instruction-following capabilities, 
    surpassing Mixtral-8x7B-Instruct. 
    SOLAR 10.7B is publicly available under the Apache 2.0 license, 
    promoting broad access and application in the LLM field.
"""
```

```
If the answer is not present in the context, please write "The information is not present in the context.
```

## Table extraction
---

`pypdf` 로 불러온 pdf 파일에서 표를 인식하지 못하는 에러가 있음

![[Pasted image 20240517194722.png]]

이를 마크다운형태로 변경하여, loader안에 넣는방식

