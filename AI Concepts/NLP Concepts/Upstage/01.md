
## Efficient fine tuning
- [[peft]]
- MEFT (Memory efficient fine tuning)
	- Q-LORA -> bitsandbytes(bnb)와 비슷
	- paged optimization -> nvidia page to page optimization


## Efficient Inference
- speculative decoding
	- 모델 두 개(main model / draft model) 사용해서 디코더에서 어려운단어는 다른 모델(draft model)이 예측하도록함
	- 동일한 [[Tokenizer]] 를 사용해야함
	- 디코딩 빨라짐

