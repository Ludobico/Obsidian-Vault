![[Pasted image 20250521172648.png]]

쿨백-라이블러 발산(Kullback-Leibler divergence) 란 **두 확률 분포 $P$ 와 $Q$ 사이의 차이를 정량화하는 척도**입니다. 수식은 다음과 같습니다.

$$
D_{KL}(P||Q) = \Sigma P(x)\log (\frac{P(x)}{Q(x)})
$$


KL Divergence는 **비대칭** 이라, A에서 B를 비교하는 점수와 B에서 A를 비교하는 점수가 다를 수 있습니다.

## KL Divergence on Machine learning
---

머신러닝에서 KL Divergence는 모델이 학습할 때 **원하는 목표에 맞게 조정**하거나 **과도한 변화를 막는 데** 자주 쓰입니다. 주요 개념으로는

1. <font color="#ffff00">분포를 목표에 맞추기</font>
- 모델이 생성한 결과(이미지, 텍스트 등)의 분포를 특정 기준 분포(이상적인 데이터 분포)에 가깝게 만들 고 싶을 때 사용
- 예 : 모델이 데이터를 압축하거나 생성할 때, 결과가 너무 엉뚱하지 않고 원하는 패턴을 따르도록 조정
- KL Divergence는 모델의 출력 분포와 목표 분포 간 차이를 계산해, 차이를 줄이도록 학습

2. <font color="#ffff00">학습 안정성 유지</font>
- 모델이 업데이트할 때, 기존의 좋은 특성을 잃지 않도록 제약을 걸 때 사용
- 예 : 모델이 새 데이터를 학습하면서도 원래 잘하던 능력을 망가지지 않게 보호
- KL Divergence는 새로운 모델의 출력 분포가 이전 분포에서 너무 멀리 벗어나지 않도록 조정하는 역할을 합니다.

3. <font color="#ffff00">복잡성 제어</font>
- 모델이 너무 복잡한 패턴을 학습하지 않도록 단순화하는 데 도움
- 데이터의 핵심만 뽑아내고 불필요한 잡음을 줄이는 데 사용
- KL Divergence는 모델의 분포가 단순한 기준(예 : 정규분포)에 맞도록 유도해 과적합을 방지

