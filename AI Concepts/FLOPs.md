- [[#LLM의 FLOPs 계산 방법|LLM의 FLOPs 계산 방법]]


![[Pasted image 20250908143249.png]]

FLOPs는 기본적으로 **부동소수점 연산 횟수를 의미**합니다. 원래는 GPU나 CPU 같은 하드웨어가 초당 얼마나 많은 부동소수점 연산을 처리할 수 있는지를 나타내는 성능 지표였지만, 최근 인공지능 연구에서는 FLOPs 라는 개념을 단순히 "초당" 성능만이 아니라, **모델 학습이나 추론에 들어가는 총 연산량**을 설명하는 단위로도 활용합니다.

어떤 논문에서 학습에 3.14e23 FLOPs가 필요하다라고 하면, 이는 모델이 전체 학습 과정에서 소모한 연산 횟수를 의미합니다.

## LLM의 FLOPs 계산 방법

FLOPs를 계산한다는 건, 모델이 학습(또는 추론)하는 동안 수행되는 부동소수점 연산의 총합을 추정하는 것을 의미합니다. 논문과 실무에서 자주 쓰이는 접근은 두 단계로 접근하는 것입니다.

- 첫째는 한 토큰(또는 한 스탭)에 대해 모델이 수행하는 연산량 (FLOPs per token)을 추정하고
- 둘째는 그 값을 전체 토큰 수(학습에 사용된 토큰 수)와 곱해서 총 학습 FLOPs를 구하는 방식입니다.

실무에서 가장 널리 쓰이는 간단한 규칙은 다음과 같습니다.

$$
\text{FLOPs} \approx 2 \text{x} (\text{모델 파라미터 수 }) \text{x}(\text{학습에 사용한 토큰 수})
$$

이 식은 [[Backward propagation]] 때문에 연산이 대략 2배라는 점과, 파라미터 수가 모델 연산 규모의 좋은 근사치라는 점을 이용한 단순화합니다.

물론 정확한 FLOPs는 모델 아키텍처, 시퀀스 길이, 레이어 구성, 어텐션 계산 방식 등 구현에 따라 달라집니다.

