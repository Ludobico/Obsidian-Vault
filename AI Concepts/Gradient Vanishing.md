
수많은 층을 연결해 학습하면 여러 난제를 해결하는 인공지능이 완성될 것 같아 보입니다. 하지만 아직 한가지 문제가 더 남아 있습니다.

![[Pasted image 20240228111024.png]]

그림과 같이 깊은 층을 만들어 보니 출력층에서 시작된 가중치 업데이트가 처음 층까지 전달되지 않는 현상이 생기는 문제가 발견되었습니다. 이는 활성화 함수로 사용된 [[Sigmoid]] 함수의 특성 때문입니다. 아래 그림과 같이 <font color="#ffff00">시그모이드 함수를 미분하면 최대치는 0.25</font>입니다. 

<font color="#92d050">시그모이드의 미분 공식</font>

$$\sigma(x) = \frac{1}{1+e^{-x}}$$
$$\sigma'(x) = \frac{d}{dx}(\frac{1}{1+e^{-x}})$$
$$\sigma'(x) = \frac{e^{-x}}{(1+e^{-x})^2}$$
$$\sigma'(x) = \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}}$$
$$\sigma'(x) = \frac{1}{1+e^{-x}} \cdot (1-\frac{1}{1+e^{-x}})$$
$$\sigma'(x) = \sigma(x) \cdot (1-\sigma(x))$$

<font color="#ffff00">1보다 작으므로 계속 곱하다 보면 0에 가까워집니다. 따라서 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기 어려워지는 것입니다.</font>

![[Pasted image 20240228132842.png]]

